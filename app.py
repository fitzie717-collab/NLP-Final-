# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nCfsumfVKvny5-uoozsFf0IdOvih0jI4
"""

import requests
import csv
import pandas as pd
from bs4 import BeautifulSoup
from langdetect import detect, DetectorFactory
from nltk.tokenize import sent_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.metrics.pairwise import cosine_similarity
import tensorflow as tf
import tensorflow_hub as hub
from transformers import TFAutoModelForQuestionAnswering, AutoTokenizer

# Ensure consistent language detection results
DetectorFactory.seed = 0

# Trusted domains for scraping
TRUSTED_DOMAINS = [
    "https://www.pewresearch.org",
    "https://www.edisonresearch.com",
    "https://www.statista.com",
    "https://www.deloitte.com",
    "https://www.mckinsey.com",
    "https://www.iab.com",
    "https://www.accenture.com",
    "https://www.thinkwithgoogle.com",
    "https://www.emarketer.com",
    "https://www.sloanreview.mit.edu",
    "https://www.arxiv.org",
    "https://www.nrf.com",
    "https://www.ama.org",
    "https://www.weforum.org",
    "https://www.nielsen.com"
]

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

# TensorFlow Universal Sentence Encoder for semantic embeddings
print("Loading TensorFlow Hub Universal Sentence Encoder...")
use_model = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")
print("USE model loaded.")

# TensorFlow-based QA model
print("Loading TensorFlow-based QA model...")
qa_tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-base-squad2")
qa_model = TFAutoModelForQuestionAnswering.from_pretrained("deepset/roberta-base-squad2")
print("QA model loaded.")

# TF-IDF and LDA for topic modeling
tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 3), stop_words="english")
lda_model = LatentDirichletAllocation(n_components=10, random_state=42)

# Fetch URL content
def fetch_with_headers(url):
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        return response
    except Exception as e:
        print(f"Error fetching {url}: {e}")
        return None

# Clean and validate URLs
def clean_url(base_url, url):
    if url.startswith("http"):
        return url
    return base_url.rstrip("/") + "/" + url.lstrip("/")

def is_relevant_url(url, title, keywords):
    return any(keyword.lower() in url.lower() or keyword.lower() in title.lower() for keyword in keywords)

def is_english(text):
    try:
        return detect(text) == "en"
    except Exception:
        return False

# Scrape reports and PDFs
def search_website_and_pdfs(base_url, query, keywords):
    search_url = f"{base_url}/?s={query}"
    response = fetch_with_headers(search_url)
    if not response:
        return [], []

    soup = BeautifulSoup(response.content, "html.parser")
    reports, pdfs = [], []
    seen_urls = set()

    for link in soup.find_all("a", href=True):
        link_url = clean_url(base_url, link["href"])
        link_text = link.text.strip()

        if link_url in seen_urls:
            continue
        seen_urls.add(link_url)

        if is_relevant_url(link_url, link_text, keywords) and is_english(link_text + link_url):
            report = {"title": link_text if link_text else "No Title", "url": link_url}
            if link_url.endswith(".pdf"):
                pdfs.append(report)
            else:
                reports.append(report)

    return reports, pdfs

# Save search results to CSV
def save_to_csv(reports, pdfs, file_path):
    with open(file_path, mode="w", newline="", encoding="utf-8") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["Type", "Title", "URL"])
        for report in reports:
            writer.writerow(["Report", report["title"], report["url"]])
        for pdf in pdfs:
            writer.writerow(["PDF", pdf["title"], pdf["url"]])
    print(f"Data saved to {file_path}.")

# Encode sentences using USE
def encode_sentences(sentences):
    return use_model(sentences).numpy()

# Tokenize content into passages
def split_into_passages(text, max_length=100):
    sentences = sent_tokenize(text)
    passages, current_passage = [], []
    word_count = 0
    for sentence in sentences:
        word_count += len(sentence.split())
        current_passage.append(sentence)
        if word_count >= max_length:
            passages.append(" ".join(current_passage))
            current_passage, word_count = [], 0
    if current_passage:
        passages.append(" ".join(current_passage))
    return passages

# Index passages for search
def index_passages(content, url):
    passages = split_into_passages(content)
    embeddings = encode_sentences(passages)
    tfidf_features = tfidf_vectorizer.fit_transform(passages)
    lda_topics = lda_model.fit_transform(tfidf_features)

    return [
        {"url": url, "passage": passage, "embedding": embedding, "topic_distribution": topic}
        for passage, embedding, topic in zip(passages, embeddings, lda_topics)
    ]

# Extract answers using TensorFlow QA model
def answer_question(question, passage):
    inputs = qa_tokenizer(question, passage, return_tensors="tf")
    outputs = qa_model(inputs)
    start_idx = tf.argmax(outputs.start_logits, axis=1).numpy()[0]
    end_idx = tf.argmax(outputs.end_logits, axis=1).numpy()[0]
    return qa_tokenizer.decode(inputs["input_ids"][0][start_idx:end_idx + 1])

# Perform semantic search
def semantic_search(query, documents, top_k=5):
    query_embedding = encode_sentences([query])
    document_embeddings = encode_sentences(documents)
    similarities = cosine_similarity(query_embedding, document_embeddings).flatten()
    top_indices = similarities.argsort()[-top_k:][::-1]
    return [(documents[i], similarities[i]) for i in top_indices]

# Process topic-based search
def process_topic_search(topic):
    keywords = topic.lower().split()
    all_reports, all_pdfs = [], []

    for domain in TRUSTED_DOMAINS:
        print(f"Searching {domain} for topic '{topic}'...")
        reports, pdfs = search_website_and_pdfs(domain, topic, keywords)
        all_reports.extend(reports)
        all_pdfs.extend(pdfs)

    save_to_csv(all_reports, all_pdfs, "search_results.csv")

    df = pd.read_csv("search_results.csv")
    urls = df["URL"].tolist()
    indexed_data = []

    for url in urls:
        print(f"Processing: {url}")
        response = fetch_with_headers(url)
        if response:
            content = BeautifulSoup(response.content, "html.parser").get_text()
            indexed_data.extend(index_passages(content, url))

    print("Performing semantic search...")
    results = semantic_search(topic, [data["passage"] for data in indexed_data])

    print("\n=== Search Results ===")
    for idx, (doc, score) in enumerate(results, start=1):
        print(f"\nResult {idx}:")
        print(f"Passage: {doc}")
        print(f"Score: {score:.4f}")

# Process specific question answering
def process_question_answering(topic, question):
    keywords = topic.lower().split()
    all_reports, all_pdfs = [], []

    for domain in TRUSTED_DOMAINS:
        print(f"Searching {domain} for topic '{topic}'...")
        reports, pdfs = search_website_and_pdfs(domain, topic, keywords)
        all_reports.extend(reports)
        all_pdfs.extend(pdfs)

    save_to_csv(all_reports, all_pdfs, "search_results.csv")

    df = pd.read_csv("search_results.csv")
    urls = df["URL"].tolist()
    indexed_data = []

    for url in urls:
        print(f"Processing: {url}")
        response = fetch_with_headers(url)
        if response:
            content = BeautifulSoup(response.content, "html.parser").get_text()
            indexed_data.extend(index_passages(content, url))

    print("Performing question answering...")
    passages = [data["passage"] for data in indexed_data]
    search_results = semantic_search(topic, passages)

    print("\n=== Question Answering Results ===")
    for idx, (doc, score) in enumerate(search_results, start=1):
        answer = answer_question(question, doc)
        print(f"\nResult {idx}:")
        print(f"Passage: {doc}")
        print(f"Score: {score:.4f}")
        print(f"Answer: {answer}")

# Main function
def main():
    mode = input("Enter '1' to search a topic or '2' to ask a specific question: ").strip()

    if mode == "1":
        topic = input("Enter the topic to search for: ").strip()
        process_topic_search(topic)
    elif mode == "2":
        topic = input("Enter the topic to search for: ").strip()
        question = input("Enter the question: ").strip()
        process_question_answering(topic, question)
    else:
        print("Invalid input. Please enter '1' or '2'.")

if __name__ == "__main__":
    main()